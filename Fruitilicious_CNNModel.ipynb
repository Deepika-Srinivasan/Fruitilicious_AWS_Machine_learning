{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "environ['TF_CPP_MIN_LOG_LEVEL'] = '3' #to suppress all tensorflow warnings\n",
    "chdir(r'C:\\Users\\Deepika\\CS5000\\Model_Callbacks_Checkpoint_SGD')\n",
    "\n",
    "from os import environ, chdir\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import models, layers, optimizers, callbacks\n",
    "from numpy import set_printoptions\n",
    "set_printoptions(precision=4,suppress=True)\n",
    "from keras.models import load_model\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Conv2D, MaxPooling2D \n",
    "from keras.layers import Dropout, Flatten, Dense \n",
    "import shutil\n",
    "import os \n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Deepika\\\\CS5000\\\\Model_Callbacks_Checkpoint_SGD'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training fresh images 1670\n",
      "Number of training rotten images 2354\n",
      "Number of Training images  4024\n",
      "********************************************************************\n",
      "Number of testing fresh images 418\n",
      "Number of testing rotten images 589\n",
      "Number of Testing images  1007\n"
     ]
    }
   ],
   "source": [
    "fresh_dirpath, fresh_dirnames, fresh_images = next(os.walk('Dataset/AppleImages/fresh'))\n",
    "rotten_dirpath, rotten_dirnames, rotten_images = next(os.walk('Dataset/AppleImages/rotten'))\n",
    "\n",
    "#Split ratio \n",
    "train_size = 0.8 \n",
    "\n",
    "#Main source folder with two folders namely fresh and rotten\n",
    "src_folder = 'Dataset/AppleImages/'\n",
    "\n",
    "# Make sure we remove any existing folders and start from a clean slate\n",
    "shutil.rmtree(src_folder+'Train/fresh/', ignore_errors=True)\n",
    "shutil.rmtree(src_folder+'Train/rotten/', ignore_errors=True)\n",
    "shutil.rmtree(src_folder+'Test/fresh/', ignore_errors=True)\n",
    "shutil.rmtree(src_folder+'Test/rotten/', ignore_errors=True)\n",
    "\n",
    "#create new empty train and test folders\n",
    "os.makedirs(src_folder+'Train/fresh/')\n",
    "os.makedirs(src_folder+'Train/rotten/')\n",
    "os.makedirs(src_folder+'Test/fresh/')\n",
    "os.makedirs(src_folder+'Test/rotten/')\n",
    "\n",
    "#Total number of fresh and rotten images\n",
    "num_fresh_images = len(fresh_images)\n",
    "num_rotten_images = len(rotten_images)\n",
    "\n",
    "#Split fresh images into training images and test images with given split ratio\n",
    "num_fresh_images_train = int(train_size * num_fresh_images)\n",
    "num_fresh_images_test = num_fresh_images - num_fresh_images_train\n",
    "\n",
    "#Split rotten images into training images and test images with given split ratio\n",
    "num_rotten_images_train = int(train_size * num_rotten_images)\n",
    "num_rotten_images_test = num_rotten_images - num_rotten_images_train\n",
    "\n",
    "# Randomly assign fresh images to train and test\n",
    "fresh_train_images = random.sample(fresh_images, num_fresh_images_train)\n",
    "for img in fresh_train_images:\n",
    "    shutil.copy(src=src_folder+'fresh/'+img, dst=src_folder+'Train/fresh/')\n",
    "#1670 images changed to #1252\n",
    "\n",
    "fresh_test_images  = [img for img in fresh_images if img not in fresh_train_images]\n",
    "for img in fresh_test_images:\n",
    "    shutil.copy(src=src_folder+'fresh/'+img, dst=src_folder+'Test/fresh/')\n",
    "#418 images changed to #836\n",
    "\n",
    "#Randomly assign rotten images to train and test\n",
    "rotten_train_images = random.sample(rotten_images, num_rotten_images_train)\n",
    "for img in rotten_train_images:\n",
    "    shutil.copy(src=src_folder+'rotten/'+img, dst=src_folder+'Train/rotten/')\n",
    "\n",
    "\n",
    "rotten_test_images  = [img for img in rotten_images if img not in rotten_train_images]\n",
    "for img in rotten_test_images:\n",
    "    shutil.copy(src=src_folder+'rotten/'+img, dst=src_folder+'Test/rotten/')\n",
    "\n",
    "    \n",
    "print(\"Number of training fresh images\",num_fresh_images_train)\n",
    "print(\"Number of training rotten images\",num_rotten_images_train)\n",
    "print(\"Number of Training images \",num_fresh_images_train+num_rotten_images_train)\n",
    "print(\"********************************************************************\")\n",
    "print(\"Number of testing fresh images\",num_fresh_images_test)\n",
    "print(\"Number of testing rotten images\",num_rotten_images_test)\n",
    "print(\"Number of Testing images \",num_fresh_images_test+num_rotten_images_test )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameters\n",
    "FILTER_SIZE = 3 \n",
    "NUM_FILTERS = 32 \n",
    "INPUT_SIZE  = 64\n",
    "MAXPOOL_SIZE = 2 \n",
    "BATCH_SIZE = 32 \n",
    "\n",
    "#Steps per epoch is based on Training so in total we have 1670 fresh and 2354 rotten images =4024/batch size=125.75 =>126\n",
    "STEP_PER_TRAIN = round((num_rotten_images_train+num_fresh_images_train)/BATCH_SIZE)\n",
    "STEP_PER_TEST= round((num_rotten_images_test+num_fresh_images_test)/BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training step size is 126,Testing step size 31\n"
     ]
    }
   ],
   "source": [
    "print(\"Training step size is {0},Testing step size {1}\".format(STEP_PER_TRAIN,STEP_PER_TEST))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4024 images belonging to 2 classes.\n",
      "Found 1007 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "#Loading Data\n",
    "#We are just gonna load the images along with data augmentation\n",
    "training_data_generator = ImageDataGenerator(rescale = 1./255,\n",
    "                                             shear_range = 0.2,\n",
    "                                             zoom_range = 0.2,\n",
    "                                             horizontal_flip = True)#AUGMENTATION DONE HERE SINCE THIS IS TRAINING IMAGE\n",
    "\n",
    "training_set = training_data_generator.flow_from_directory('Dataset/AppleImages/Train/',\n",
    "                                                           target_size=(INPUT_SIZE,INPUT_SIZE),                                                    \n",
    "                                                           batch_size=BATCH_SIZE,                                                    \n",
    "                                                           class_mode='binary')\n",
    "\n",
    "testing_data_generator = ImageDataGenerator(rescale = 1./255)#NO MUCH AUGMENTATION DONE HERE-NO CHANGE\n",
    "\n",
    "test_set = testing_data_generator.flow_from_directory('Dataset/AppleImages/Test/',                               \n",
    "                                                      target_size=(INPUT_SIZE,INPUT_SIZE),                        \n",
    "                                                      batch_size=BATCH_SIZE,                                   \n",
    "                                                      class_mode = 'binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "#Model 4 (conv+relu+maxpool)(conv+relu+maxpool)(Dense 128)(Dense 1)\n",
    "model = Sequential() \n",
    "\n",
    "model.add(Conv2D(NUM_FILTERS, (FILTER_SIZE, FILTER_SIZE),  \n",
    "                 input_shape = (INPUT_SIZE, INPUT_SIZE, 3),                \n",
    "                 activation = 'relu')) \n",
    "\n",
    "#we add a max pooling layer:\n",
    "model.add(MaxPooling2D(pool_size = (MAXPOOL_SIZE, MAXPOOL_SIZE))) \n",
    "\n",
    "#Repeat 2nd convolutional layer\n",
    "model.add(Conv2D(NUM_FILTERS, (FILTER_SIZE, FILTER_SIZE),  \n",
    "                 input_shape = (INPUT_SIZE, INPUT_SIZE, 3),                \n",
    "                 activation = 'relu')) \n",
    "\n",
    "#we add a max pooling layer:\n",
    "model.add(MaxPooling2D(pool_size = (MAXPOOL_SIZE, MAXPOOL_SIZE))) \n",
    "\n",
    "#flatten multi dimensional array to single to 1dimensional vector\n",
    "model.add(Flatten())\n",
    "\n",
    "#add a fully connected layer with 128 nodes:\n",
    "model.add(Dense(units = 128, activation = 'relu'))\n",
    "\n",
    "#one last fully connected layer to our model:\n",
    "model.add(Dense(units = 1, activation = 'sigmoid')) \n",
    "\n",
    "\n",
    "\n",
    "# Model Loss function and Optimizer SGD with lr =0.15 method \n",
    "compile = model.compile(optimizer=optimizers.sgd(lr=0.15), loss='binary_crossentropy',\n",
    "                           metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_5 (Conv2D)            (None, 62, 62, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 31, 31, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 29, 29, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 128)               802944    \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 813,217\n",
      "Trainable params: 813,217\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settting Callbacks\n",
    "check_p = callbacks.ModelCheckpoint(filepath='FruitStatus_cnn_{val_acc:.2f}.h5',\n",
    "                                    monitor='val_acc', verbose=1,\n",
    "                                    save_best_only=True, save_weights_only=False)\n",
    "\n",
    "\n",
    "#2nd Callback\n",
    "reduce_lr = callbacks.ReduceLROnPlateau(monitor='val_acc', factor=0.95, patience=3,\n",
    "                                        verbose=1, cooldown=2)\n",
    "\n",
    "#list two callbacks \n",
    "callb_l = [check_p, reduce_lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Deepika\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "Epoch 1/50\n",
      "126/126 [==============================] - 29s 230ms/step - loss: 0.6861 - acc: 0.5820 - val_loss: 0.6776 - val_acc: 0.5857\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.58569, saving model to FruitStatus_cnn_0.59.h5\n",
      "Epoch 2/50\n",
      "126/126 [==============================] - 24s 194ms/step - loss: 0.6737 - acc: 0.5964 - val_loss: 0.6800 - val_acc: 0.5815\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.58569\n",
      "Epoch 3/50\n",
      "126/126 [==============================] - 25s 195ms/step - loss: 0.6714 - acc: 0.5907 - val_loss: 0.6814 - val_acc: 0.5836\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.58569\n",
      "Epoch 4/50\n",
      "126/126 [==============================] - 25s 198ms/step - loss: 0.6764 - acc: 0.5845 - val_loss: 0.6660 - val_acc: 0.5856\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.58569\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.14250000566244125.\n",
      "Epoch 5/50\n",
      "126/126 [==============================] - 26s 203ms/step - loss: 0.6664 - acc: 0.5975 - val_loss: 0.6868 - val_acc: 0.5856\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.58569\n",
      "Epoch 6/50\n",
      "126/126 [==============================] - 25s 197ms/step - loss: 0.6793 - acc: 0.5805 - val_loss: 0.6748 - val_acc: 0.5856\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.58569\n",
      "Epoch 7/50\n",
      "126/126 [==============================] - 25s 197ms/step - loss: 0.6724 - acc: 0.5921 - val_loss: 0.6542 - val_acc: 0.6738\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.58569 to 0.67385, saving model to FruitStatus_cnn_0.67.h5\n",
      "Epoch 8/50\n",
      "126/126 [==============================] - 27s 212ms/step - loss: 0.6686 - acc: 0.6009 - val_loss: 0.6153 - val_acc: 0.7179\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.67385 to 0.71795, saving model to FruitStatus_cnn_0.72.h5\n",
      "Epoch 9/50\n",
      "126/126 [==============================] - 25s 201ms/step - loss: 0.6688 - acc: 0.5998 - val_loss: 0.6757 - val_acc: 0.5887\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.71795\n",
      "Epoch 10/50\n",
      "126/126 [==============================] - 26s 207ms/step - loss: 0.6688 - acc: 0.6048 - val_loss: 0.6763 - val_acc: 0.5938\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.71795\n",
      "Epoch 11/50\n",
      "126/126 [==============================] - 25s 198ms/step - loss: 0.6793 - acc: 0.5858 - val_loss: 0.6765 - val_acc: 0.5887\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.71795\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.13537501245737074.\n",
      "Epoch 12/50\n",
      "126/126 [==============================] - 24s 192ms/step - loss: 0.6788 - acc: 0.5872 - val_loss: 0.6742 - val_acc: 0.5949\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.71795\n",
      "Epoch 13/50\n",
      "126/126 [==============================] - 25s 202ms/step - loss: 0.6784 - acc: 0.5880 - val_loss: 0.6772 - val_acc: 0.5815\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.71795\n",
      "Epoch 14/50\n",
      "126/126 [==============================] - 25s 196ms/step - loss: 0.6776 - acc: 0.5873 - val_loss: 0.6786 - val_acc: 0.5723\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.71795\n",
      "Epoch 15/50\n",
      "126/126 [==============================] - 25s 195ms/step - loss: 0.6773 - acc: 0.5878 - val_loss: 0.6713 - val_acc: 0.5979\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.71795\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.12860625758767127.\n",
      "Epoch 16/50\n",
      "126/126 [==============================] - 25s 195ms/step - loss: 0.6771 - acc: 0.5882 - val_loss: 0.6723 - val_acc: 0.5897\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.71795\n",
      "Epoch 17/50\n",
      "126/126 [==============================] - 25s 195ms/step - loss: 0.6642 - acc: 0.5960 - val_loss: 0.6490 - val_acc: 0.6031\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.71795\n",
      "Epoch 18/50\n",
      "126/126 [==============================] - 25s 198ms/step - loss: 0.6541 - acc: 0.6221 - val_loss: 0.5839 - val_acc: 0.7272\n",
      "\n",
      "Epoch 00018: val_acc improved from 0.71795 to 0.72718, saving model to FruitStatus_cnn_0.73.h5\n",
      "Epoch 19/50\n",
      "126/126 [==============================] - 25s 199ms/step - loss: 0.6167 - acc: 0.6594 - val_loss: 0.6408 - val_acc: 0.5928\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.72718\n",
      "Epoch 20/50\n",
      "126/126 [==============================] - 25s 195ms/step - loss: 0.5635 - acc: 0.7017 - val_loss: 0.5373 - val_acc: 0.7477\n",
      "\n",
      "Epoch 00020: val_acc improved from 0.72718 to 0.74769, saving model to FruitStatus_cnn_0.75.h5\n",
      "Epoch 21/50\n",
      "126/126 [==============================] - 25s 197ms/step - loss: 0.5336 - acc: 0.7321 - val_loss: 0.5407 - val_acc: 0.7097\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.74769\n",
      "Epoch 22/50\n",
      "126/126 [==============================] - 25s 197ms/step - loss: 0.5157 - acc: 0.7384 - val_loss: 0.5078 - val_acc: 0.7436\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.74769\n",
      "Epoch 23/50\n",
      "126/126 [==============================] - 25s 196ms/step - loss: 0.4903 - acc: 0.7501 - val_loss: 0.4414 - val_acc: 0.7795\n",
      "\n",
      "Epoch 00023: val_acc improved from 0.74769 to 0.77949, saving model to FruitStatus_cnn_0.78.h5\n",
      "Epoch 24/50\n",
      "126/126 [==============================] - 27s 210ms/step - loss: 0.4833 - acc: 0.7606 - val_loss: 0.3362 - val_acc: 0.8585\n",
      "\n",
      "Epoch 00024: val_acc improved from 0.77949 to 0.85846, saving model to FruitStatus_cnn_0.86.h5\n",
      "Epoch 25/50\n",
      "126/126 [==============================] - 24s 194ms/step - loss: 0.4285 - acc: 0.7975 - val_loss: 0.6042 - val_acc: 0.6410\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.85846\n",
      "Epoch 26/50\n",
      "126/126 [==============================] - 25s 195ms/step - loss: 0.4035 - acc: 0.8176 - val_loss: 0.3552 - val_acc: 0.8462\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.85846\n",
      "Epoch 27/50\n",
      "126/126 [==============================] - 25s 196ms/step - loss: 0.4115 - acc: 0.8103 - val_loss: 0.3799 - val_acc: 0.8256\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.85846\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 0.12217594683170319.\n",
      "Epoch 28/50\n",
      "126/126 [==============================] - 24s 194ms/step - loss: 0.3077 - acc: 0.8619 - val_loss: 0.3226 - val_acc: 0.8646\n",
      "\n",
      "Epoch 00028: val_acc improved from 0.85846 to 0.86462, saving model to FruitStatus_cnn_0.86.h5\n",
      "Epoch 29/50\n",
      "126/126 [==============================] - 25s 196ms/step - loss: 0.2768 - acc: 0.8843 - val_loss: 0.2567 - val_acc: 0.8851\n",
      "\n",
      "Epoch 00029: val_acc improved from 0.86462 to 0.88513, saving model to FruitStatus_cnn_0.89.h5\n",
      "Epoch 30/50\n",
      "126/126 [==============================] - 25s 197ms/step - loss: 0.2360 - acc: 0.9057 - val_loss: 0.2053 - val_acc: 0.9200\n",
      "\n",
      "Epoch 00030: val_acc improved from 0.88513 to 0.92000, saving model to FruitStatus_cnn_0.92.h5\n",
      "Epoch 31/50\n",
      "126/126 [==============================] - 25s 195ms/step - loss: 0.2467 - acc: 0.8957 - val_loss: 0.1360 - val_acc: 0.9426\n",
      "\n",
      "Epoch 00031: val_acc improved from 0.92000 to 0.94256, saving model to FruitStatus_cnn_0.94.h5\n",
      "Epoch 32/50\n",
      "126/126 [==============================] - 25s 195ms/step - loss: 0.1966 - acc: 0.9223 - val_loss: 0.1631 - val_acc: 0.9251\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.94256\n",
      "Epoch 33/50\n",
      "126/126 [==============================] - 25s 195ms/step - loss: 0.2385 - acc: 0.9086 - val_loss: 0.1440 - val_acc: 0.9456\n",
      "\n",
      "Epoch 00033: val_acc improved from 0.94256 to 0.94556, saving model to FruitStatus_cnn_0.95.h5\n",
      "Epoch 34/50\n",
      "126/126 [==============================] - 25s 199ms/step - loss: 0.1622 - acc: 0.9320 - val_loss: 0.1281 - val_acc: 0.9518\n",
      "\n",
      "Epoch 00034: val_acc improved from 0.94556 to 0.95179, saving model to FruitStatus_cnn_0.95.h5\n",
      "Epoch 35/50\n",
      "126/126 [==============================] - 25s 194ms/step - loss: 0.2158 - acc: 0.9132 - val_loss: 0.1296 - val_acc: 0.9405\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.95179\n",
      "Epoch 36/50\n",
      "126/126 [==============================] - 25s 197ms/step - loss: 0.1679 - acc: 0.9336 - val_loss: 0.1571 - val_acc: 0.9374\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.95179\n",
      "Epoch 37/50\n",
      "126/126 [==============================] - 25s 198ms/step - loss: 0.1193 - acc: 0.9488 - val_loss: 0.1008 - val_acc: 0.9631\n",
      "\n",
      "Epoch 00037: val_acc improved from 0.95179 to 0.96308, saving model to FruitStatus_cnn_0.96.h5\n",
      "Epoch 38/50\n",
      "126/126 [==============================] - 24s 194ms/step - loss: 0.1072 - acc: 0.9585 - val_loss: 0.1005 - val_acc: 0.9621\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.96308\n",
      "Epoch 39/50\n",
      "126/126 [==============================] - 24s 192ms/step - loss: 0.1068 - acc: 0.9587 - val_loss: 0.0736 - val_acc: 0.9774\n",
      "\n",
      "Epoch 00039: val_acc improved from 0.96308 to 0.97744, saving model to FruitStatus_cnn_0.98.h5\n",
      "Epoch 40/50\n",
      "126/126 [==============================] - 24s 194ms/step - loss: 0.0979 - acc: 0.9611 - val_loss: 0.1218 - val_acc: 0.9497\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.97744\n",
      "Epoch 41/50\n",
      "126/126 [==============================] - 25s 194ms/step - loss: 0.0954 - acc: 0.9633 - val_loss: 0.0500 - val_acc: 0.9836\n",
      "\n",
      "Epoch 00041: val_acc improved from 0.97744 to 0.98359, saving model to FruitStatus_cnn_0.98.h5\n",
      "Epoch 42/50\n",
      "126/126 [==============================] - 24s 193ms/step - loss: 0.0943 - acc: 0.9625 - val_loss: 0.0839 - val_acc: 0.9754\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.98359\n",
      "Epoch 43/50\n",
      "126/126 [==============================] - 24s 193ms/step - loss: 0.1009 - acc: 0.9623 - val_loss: 0.1191 - val_acc: 0.9477\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.98359\n",
      "Epoch 44/50\n",
      "126/126 [==============================] - 24s 194ms/step - loss: 0.0809 - acc: 0.9679 - val_loss: 0.0635 - val_acc: 0.9713\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.98359\n",
      "\n",
      "Epoch 00044: ReduceLROnPlateau reducing learning rate to 0.11606714949011802.\n",
      "Epoch 45/50\n",
      "126/126 [==============================] - 24s 193ms/step - loss: 0.0772 - acc: 0.9736 - val_loss: 0.1000 - val_acc: 0.9662\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.98359\n",
      "Epoch 46/50\n",
      "126/126 [==============================] - 24s 193ms/step - loss: 0.0779 - acc: 0.9677 - val_loss: 0.0468 - val_acc: 0.9815\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.98359\n",
      "Epoch 47/50\n",
      "126/126 [==============================] - 24s 193ms/step - loss: 0.0628 - acc: 0.9745 - val_loss: 0.0694 - val_acc: 0.9754\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.98359\n",
      "Epoch 48/50\n",
      "126/126 [==============================] - 24s 192ms/step - loss: 0.0848 - acc: 0.9687 - val_loss: 0.1277 - val_acc: 0.9528\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.98359\n",
      "\n",
      "Epoch 00048: ReduceLROnPlateau reducing learning rate to 0.11026379130780696.\n",
      "Epoch 49/50\n",
      "126/126 [==============================] - 24s 192ms/step - loss: 0.0649 - acc: 0.9762 - val_loss: 0.0661 - val_acc: 0.9764\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.98359\n",
      "Epoch 50/50\n",
      "126/126 [==============================] - 24s 193ms/step - loss: 0.0509 - acc: 0.9813 - val_loss: 0.0498 - val_acc: 0.9867\n",
      "\n",
      "Epoch 00050: val_acc improved from 0.98359 to 0.98667, saving model to FruitStatus_cnn_0.99.h5\n"
     ]
    }
   ],
   "source": [
    "#Fit model\n",
    "#fit_generator deals directly with images \n",
    "history = model.fit_generator(generator=training_set, \n",
    "                             steps_per_epoch=STEP_PER_TRAIN, \n",
    "                             epochs=50, verbose=1,\n",
    "                             callbacks=callb_l, \n",
    "                             validation_data=test_set, \n",
    "                             validation_steps=STEP_PER_TEST)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fruit_Status_CNN last layer bias:\n",
      "[-0.5858]\n",
      "Fruit_Status_CNN last layer weights:\n",
      "[[ 0.0337]\n",
      " [-0.3784]\n",
      " [-0.3113]\n",
      " [ 0.0095]\n",
      " [-0.384 ]\n",
      " [-0.1373]\n",
      " [-0.3862]\n",
      " [ 0.1123]\n",
      " [ 0.0669]\n",
      " [-0.2031]\n",
      " [ 0.2189]\n",
      " [ 0.2736]\n",
      " [-0.0652]\n",
      " [ 0.1827]\n",
      " [-0.2007]\n",
      " [ 0.0759]\n",
      " [ 0.0825]\n",
      " [-0.1931]\n",
      " [-0.5113]\n",
      " [ 0.0917]\n",
      " [ 0.0705]\n",
      " [ 0.1971]\n",
      " [-0.1184]\n",
      " [-0.2954]\n",
      " [-0.2396]\n",
      " [-0.3844]\n",
      " [ 0.2199]\n",
      " [-0.2247]\n",
      " [ 0.4313]\n",
      " [ 0.1208]\n",
      " [-0.1383]\n",
      " [ 0.1679]\n",
      " [ 0.0865]\n",
      " [-0.3672]\n",
      " [-0.081 ]\n",
      " [-0.1232]\n",
      " [ 0.0596]\n",
      " [-0.2494]\n",
      " [-0.0695]\n",
      " [-0.0975]\n",
      " [-0.0821]\n",
      " [-0.2538]\n",
      " [ 0.1669]\n",
      " [-0.129 ]\n",
      " [-0.3372]\n",
      " [ 0.0423]\n",
      " [-0.3057]\n",
      " [-0.6299]\n",
      " [ 0.079 ]\n",
      " [-0.2573]\n",
      " [-0.0583]\n",
      " [-0.2519]\n",
      " [-0.23  ]\n",
      " [ 0.2152]\n",
      " [ 0.0625]\n",
      " [ 0.2435]\n",
      " [-0.5728]\n",
      " [-0.314 ]\n",
      " [-0.1856]\n",
      " [ 0.0936]\n",
      " [-0.2879]\n",
      " [-0.2592]\n",
      " [-0.1485]\n",
      " [ 0.0185]\n",
      " [ 0.1603]\n",
      " [-0.0662]\n",
      " [ 0.195 ]\n",
      " [ 0.1209]\n",
      " [ 0.1336]\n",
      " [-0.1057]\n",
      " [ 0.154 ]\n",
      " [ 0.0086]\n",
      " [ 0.0843]\n",
      " [ 0.1496]\n",
      " [-0.3472]\n",
      " [-0.2777]\n",
      " [-0.2308]\n",
      " [-0.5261]\n",
      " [ 0.1986]\n",
      " [ 0.0588]\n",
      " [-0.0358]\n",
      " [-0.2237]\n",
      " [ 0.1599]\n",
      " [ 0.0811]\n",
      " [ 0.224 ]\n",
      " [ 0.0655]\n",
      " [ 0.0667]\n",
      " [-0.1201]\n",
      " [ 0.3728]\n",
      " [-0.0232]\n",
      " [ 0.1271]\n",
      " [-0.0676]\n",
      " [ 0.0161]\n",
      " [-0.4279]\n",
      " [ 0.038 ]\n",
      " [-0.4588]\n",
      " [-0.0311]\n",
      " [-0.3855]\n",
      " [-0.2386]\n",
      " [-0.3312]\n",
      " [-0.22  ]\n",
      " [-0.1901]\n",
      " [-0.2085]\n",
      " [-0.0227]\n",
      " [ 0.1759]\n",
      " [-0.1777]\n",
      " [ 0.0487]\n",
      " [-0.2426]\n",
      " [ 0.0608]\n",
      " [-0.0681]\n",
      " [-0.2753]\n",
      " [-0.2114]\n",
      " [-0.2436]\n",
      " [ 0.0762]\n",
      " [ 0.0745]\n",
      " [-0.2521]\n",
      " [ 0.2223]\n",
      " [-0.2638]\n",
      " [ 0.02  ]\n",
      " [ 0.0687]\n",
      " [ 0.1974]\n",
      " [ 0.3914]\n",
      " [-0.035 ]\n",
      " [ 0.1493]\n",
      " [ 0.0573]\n",
      " [ 0.157 ]\n",
      " [-0.2715]\n",
      " [-0.2067]]\n"
     ]
    }
   ],
   "source": [
    "# Loading best Model\n",
    "my_model = load_model(filepath=r'C:\\Users\\Deepika\\CS5000\\Model_Callbacks_Checkpoint_SGD\\FruitStatus_cnn_0.99.h5')\n",
    "\n",
    "# Parameters: Weights and Biases\n",
    "print('Fruit_Status_CNN last layer bias:')\n",
    "print(my_model.get_weights()[-1])\n",
    "print('Fruit_Status_CNN last layer weights:')\n",
    "print(my_model.get_weights()[-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10 images belonging to 2 classes.\n",
      "Evaluation loss over Test dataset is :0.1344\n",
      "Evaluation accuracy over Test dataset is :90.00%\n"
     ]
    }
   ],
   "source": [
    "#Model Evaluation using folder containing images saved in a folder.this folder has fresh and rotten in two folders \n",
    "#dont modify image just rescale them\n",
    "eval_idg=ImageDataGenerator(rescale=1. / 255)#no need image augmentation\n",
    "\n",
    "#pull images from test folder which has 5 each for respective class .Thus batch size is 10\n",
    "eval_g=eval_idg.flow_from_directory(directory=r'C:\\Users\\Deepika\\CS5000\\Model_Callbacks_Checkpoint_SGD\\Dataset\\AppleImages\\cnn_apple_realtest_internet',\n",
    "                                    target_size=(INPUT_SIZE,INPUT_SIZE),                                                    \n",
    "                                    class_mode='binary',    \n",
    "                                    batch_size=10,#number of images in cnn_apple_realtest folder\n",
    "                                    shuffle=False)#we are going to pull images from one class and next class is used \n",
    "\n",
    "#apply test over 10 images at once \n",
    "(eval_loss,eval_acc)=my_model.evaluate_generator(generator=eval_g,steps=1)\n",
    "print('Evaluation loss over Test dataset is :{:.4f}'.format(eval_loss))\n",
    "print('Evaluation accuracy over Test dataset is :{:4.2f}%'.format(eval_acc*100))\n",
    "#print(eval_loss,eval_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "['fresh\\\\fresh_internet_1.jpg', 'fresh\\\\fresh_internet_2.jpg', 'fresh\\\\fresh_internet_3.jpg', 'fresh\\\\fresh_internet_4.jpg', 'fresh\\\\fresh_internet_5.jpg', 'rotten\\\\rotten_internet_1.jpg', 'rotten\\\\rotten_internet_2.jpg', 'rotten\\\\rotten_internet_3.jpg', 'rotten\\\\rotten_internet_4.jpg', 'rotten\\\\rotten_internet_5.jpg'] \n",
      "\n",
      "{'fresh': 0, 'rotten': 1} \n",
      "\n",
      "[[0.0953]\n",
      " [0.0026]\n",
      " [0.    ]\n",
      " [0.    ]\n",
      " [0.    ]] \n",
      "\n",
      "[[1.    ]\n",
      " [1.    ]\n",
      " [1.    ]\n",
      " [1.    ]\n",
      " [0.2892]] \n",
      "\n",
      "[[ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]] \n",
      "\n",
      "[[ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [False]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Individual Prediction\n",
    "#we are using same test dataset so make use of same eval_idg,eval_g\n",
    "pred_idg=eval_idg\n",
    "pred_g=eval_g\n",
    "\n",
    "#10 individual prediction same time\n",
    "pred=my_model.predict_generator(generator=pred_g,steps=1)#go though one pciture at a time\n",
    "\n",
    "print('\\n')\n",
    "print(pred_g.filenames,'\\n')\n",
    "print(pred_g.class_indices,'\\n')\n",
    "print(pred[0:5],'\\n')\n",
    "print(pred[5:10],'\\n')\n",
    "\n",
    "print(pred[0:5]<0.5,'\\n')\n",
    "print(pred[5:10]>0.5,'\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict single rotten image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename='Dataset/AppleImages/cnn_apple_realtest_internet/rotten/rotten_internet_7.jpg'\n",
    "#filename='Dataset/AppleImages/cnn_apple_realtest_internet/fresh/fresh_internet_3.jpg'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=64x64 at 0x23901C2AC18>"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Trying single image\n",
    "test_image = image.load_img(filename, target_size = (64, 64,3))\n",
    "test_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My model predicted as [[1.]]\n",
      "Apple Test Image is predicted as rotten \n",
      "Apple is predicted as Rotten with probability 100.00%\n"
     ]
    }
   ],
   "source": [
    "test_image = image.img_to_array(test_image)\n",
    "test_image=test_image/255\n",
    "test_image = numpy.expand_dims(test_image, axis = 0)\n",
    "result = my_model.predict(test_image) #result values are in decimals so using np.around to get binary values\n",
    "print(\"My model predicted as\",result)\n",
    "\n",
    "if result[0][0] >= 0.2:\n",
    "    prediction = 'rotten'\n",
    "else:\n",
    "    prediction = 'fresh'\n",
    "    \n",
    "prob=result[0][0]\n",
    "if prob >= 0.2:\n",
    "    prediction = 'rotten'\n",
    "    print(\"Apple Test Image is predicted as {} \".format(prediction))\n",
    "    print(\"Apple is predicted as Rotten with probability {:4.2f}%\".format(prob*100))\n",
    "else:\n",
    "    prediction = 'fresh'\n",
    "    print(\"Apple Test Image is predicted as {} \".format(prediction))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
